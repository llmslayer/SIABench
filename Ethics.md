# Ethics Statement

We decided to publish our SIA benchmark dataset, autonomous evaluation agent, and results publicly. Specifically, we first discuss our compliance with the terms and conditions (T&C) of the platforms from which we curated our data. Second, we identify both positive and negative outcomes that may arise from making the work publicly available, the risk mitigation steps taken, and how we weighed the benefits against the negative outcomes to arrive at this decision.

## Data Curation and Compliance

Our SIA dataset has been developed by carefully curating security incident-related challenges from three blue team platforms: [BlueTeamLabsOnline](https://blueteamlabs.online/), [CyberDefenders](https://cyberdefenders.org/), and [TryHackMe](https://tryhackme.com/). Before data curation, we carefully reviewed their T&C and complied with them by taking several steps, as detailed below. Additionally, we reviewed prior leading works, such as [PentestGPT](https://www.usenix.org/conference/usenixsecurity24/presentation/deng) and [InterCode-CTF](https://openreview.net/forum?id=KOZwk7BFc3), who curated data from similar platforms as ours, and tailored their best practices to our released SIA dataset. PentestGPT and InterCode-CTF used challenges from [HackTheBox](https://www.hackthebox.com/) and [PicoCTF](https://picoctf.org/), which abide by similar T&C as ours. PentestGPT used multiple active labs from HackTheBox in their benchmark dataset and provided the names of the labs, allowing users instantiate the labs and run experiments. Since PentestGPT used active labs, all of them require login and some of them require paid subscriptions as well. The InterCode-CTF data file (.json) contains the CTF challenges (query and answer) and corresponding source URL referring to the original platform, and in some cases the URLs of the write-ups where they curated the solutions from.

Our problem set takes a similar approach. We provide the SIA challenges within our framework (e.g., scenario, questions, answers), but do not host files/artifacts directly and instead provide source URLs to access or download the files/artifacts from. Accessing the files/artifacts from the source platforms may require login, similar to the challenges from [HackTheBox](https://www.hackthebox.com/) and [PicoCTF](https://picoctf.org/), but do not require any subscription or payment. All SIA problems in our dataset are either retired or free; have legit and multiple open-source [write-ups](https://github.com/Panagiotis-INS/Cyber-Defenders/tree/main) (under AGPL 3.0 license or open-source blogs) containing the scenario, questions, solving strategy, and answers. We provide URLs to all publicly available write-ups for our problem sets so users can verify the solutions. All these steps will facilitate the reproduction of the results without violating the T&C of the platforms. That said, some files/artifacts involved may contain potentially harmful content. We strongly recommend executing the code and downloading artifacts only within isolated environments, such as a Kali Linux virtual machine, to ensure system safety.

## Benefits and Positive Outcomes

By establishing an evaluation benchmark, our work highlights both the strengths and weaknesses of large language models (LLMs) in SIA tasks. It provides insights into where these models excel, where they struggle, and offers analyses of the reasons behind their failuresâ€”ultimately identifying areas that need further improvement to perform SIA tasks. While these findings are valuable, we acknowledge that some stakeholders may perceive them as a deterrent to rapid AI adoption, especially since the evaluation data clearly show that some models outperform others, and in certain contexts LLMs may not yet be mature enough. However, hasty adoption can backfire, resulting in financial losses, reputational harm, or even legal issues, particularly given the high-stakes nature of deploying AI in cyber defence. Therefore, our goal is to help the stakeholders make better-informed decisions by understanding the capabilities and limitations of LLMs in performing SIA tasks, thereby ensuring a more reliable return on investment. This knowledge also empowers them to critically assess AI tools, identify potential remedies or workarounds, and adopt them more responsibly. Interestingly, commercial companies like Anthropic have also recognized the value of public benchmarks, such as [Cybench](https://arxiv.org/abs/2408.08926), using them alongside internal evaluations and publishing the results in their reports. Having accessible means of independent validation fosters greater trust in AI models as well as the products built on top of them - leading to more robust and reliable use of AI in the long run.

## Risk Assessment and Mitigation

We also acknowledge the potential risk of misuse associated with releasing our data, agent, and results. Malicious actors could adapt the agent for adversarial purpose. The results as well could inform them about the current capabilities of LLMs in solving SIA tasks, which knowledge could be exploited to deceive cyber defenders. However, given the existence of publicly available Capture The Flag (CTF) and penetration testing benchmarks and agents, we believe the risk of adapting our agent for adversarial purpose is low. Additionally, malicious actors may obtain similar technology through covert means. For these reasons, we believe accelerating cyber defense through advanced technologies like LLMs is critical. Our results track the progress of current LLMs on SIA tasks, identifies gaps, which would accelerate and foster the development of stronger AI agents for cyber defense.

## Transparency and Reproducibility

In addition, as researchers, we prioritize transparency and reproducibility. By sharing our data, agent, and results, we aim to support the broader community in validating and building upon our work. Although due to the [inherent variability](https://arxiv.org/abs/2501.19393) in LLM outputs, full replication of our results may not always be possible.

## Conclusion

After weighing the benefits such as advancing progress, informing stakeholders and policymakers, and enabling transparency and reproducibility over potential risks, we decided to publicly release our dataset, agent, and results. Note that the agent source code will be available upon acceptance of the paper.